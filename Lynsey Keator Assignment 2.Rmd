---
title: "Assignment 2: Chapter 3"
author: "Lynsey Keator"
date: "May 26, 2020"
output: html_notebook
---

## 3.16 
*Children were asked to build towers out of cubical and cylindrical blocks as high as they could, and the number of blocks used and the time taken were recorded. The data and a fuller description are given in Problem 2.18. In that problem, a linear regression model was fitted to model the time to build the towers, based on the initial examination in Problem 1.9.*

```{r}
# Load libraries and data

library(tidyverse)
library(GLMsData)
data(blocks)

# Make trial a factor

blocks$Trial <- factor(blocks$Trial,
                       levels=c(1,2),
                       labels=c("1", "2"))
```


**1. Perform a diagnostic analysis of the linear regression model fitted in Problem 2.18, and show a transformation of the response is necessary.**

```{r}
# Load model from 2.18

model <- lm(Time ~ Shape, data=blocks)
anova(model)
```

```{r}
# Calculate raw and standardized residuals

resid.raw <- resid(model)
resid.std <- rstandard(model)

c(Raw =var(resid.raw), Standardized = var(resid.std))
```
I see that the standardized residuals are close to 1. This makes sense because they are from a distribution that is close to a standard normal and the variance should be ~ 1.

Next, I will calculate fitted values and look at plots of residuals as function of fitted values:

```{r}
fit.values <- fitted(model)

plot(fit.values, resid.raw)
plot(fit.values, resid.std)
```


*Check for __independence__ of the responses when possible. This assumption can be hard to check, as this may depend on the method of data collection. However, if the data are collected over time, dependence may be identified by plotting residuals against the previous residual in time. If data are spatial, check for dependence by plot residuals against spatial variables.*

*Check for __linearity__ between the responses and all covariates using plots of the residuals against each explanatory variable. Linearity between the response and explanatory variables after adjusting for the effects of the other explanatory variables can also be assessed using partial residual plots.*

```{r}


pterms <- predict(model, type = "terms")
partial.resid <- apply(pterms, 2, function(x) x+resid(model))
head(partial.resid[, 1])
```

```{r}
# Look at partial residual plots. We can obtain these be regressing the response variable on the explanatory variable.

termplot(model,
         partial.resid = TRUE,
         terms = "Shape",
         las = 1)
```

*Check for __constant variance__ of the response variable using plots of the residuals against mu hat.*

```{r}
# To look at constant variance, look at it in the context of a full model. Plot standardized residuals as a functionl of predicted or fitted values.

scatter.smooth(rstandard(model) ~ fitted(model),
               col = "grey", 
               las = 1,
               ylab = "Standardized residuals",
               xlab = "Fitted values")
```
Plots above show variance differs between cubes and cylinders.

*Check for normality of the responses using a __Q-Q plot__.*

```{r}
qqnorm(rstandard(model),
       las = 1,
       pch = 19)

qqline(rstandard(model))
```
This plot suggests heavier tails than expected if the data are normally distributed. 

```{r}
plot(cooks.distance(model), type = "h")
plot(rstandard(model) ~ blocks$Shape)
rowSums(influence.measures(model)$is.inf)
```


**2. Fit an appropriate linear regression model to the data after applying the transformation, ensuring a diagnostic analysis.**